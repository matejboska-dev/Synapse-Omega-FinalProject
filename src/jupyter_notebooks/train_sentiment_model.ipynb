{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trénování modelu pro analýzu sentimentu\n",
    "\n",
    "Tento notebook implementuje trénink neuronového modelu pro analýzu sentimentu českých zpravodajských článků."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# TensorFlow a Keras pro neuronovou síť\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scikit-learn pro metriky a dělení dat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nastavení cest a logování"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# add parent directory to system path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n",
    "# ensure log directory exists\n",
    "log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('.')))), 'logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"train_sentiment_model_neural_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "\n",
    "# absolute imports from src directory\n",
    "from models.sentiment_analyzer import SentimentAnalyzer\n",
    "\n",
    "# logger configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode='w'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definice třídy neuronového analyzátoru sentimentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class NeuralSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Sentiment Analyzer založený na neuronové síti LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000, embedding_dim=128, max_length=500):\n",
    "        \"\"\"\n",
    "        Inicializace analyzátoru sentimentu s neuronovou sítí\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Velikost slovníku (počet nejčastějších slov)\n",
    "            embedding_dim (int): Dimenze vektorů slov (embedding)\n",
    "            max_length (int): Maximální délka článku (počet slov)\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "        self.model = None\n",
    "        self.labels = ['negative', 'neutral', 'positive']\n",
    "        self.history = None\n",
    "        \n",
    "        # Načtení českých pozitivních a negativních slov pro vysvětlení\n",
    "        self.positive_words = [\n",
    "            'dobrý', 'skvělý', 'výborný', 'pozitivní', 'úspěch', 'radost', 'krásný', 'příjemný',\n",
    "            'štěstí', 'spokojený', 'výhra', 'zisk', 'růst', 'lepší', 'nejlepší', 'zlepšení',\n",
    "            'výhoda', 'prospěch', 'podpora', 'rozvoj', 'pokrok', 'úspěšný', 'optimistický',\n",
    "            'šťastný', 'veselý', 'bezpečný', 'klidný', 'prospěšný', 'úžasný', 'perfektní',\n",
    "            'vynikající', 'senzační', 'fantastický', 'neuvěřitelný', 'báječný', 'nádherný',\n",
    "            'velkolepý', 'luxusní', 'přátelský', 'laskavý', 'milý', 'ochotný', 'talentovaný',\n",
    "            'nadaný', 'inovativní', 'kreativní', 'silný', 'výkonný', 'efektivní', 'užitečný',\n",
    "            'cenný', 'důležitý', 'ohromující', 'fascinující', 'zajímavý', 'pozoruhodný',\n",
    "            'inspirativní', 'motivující', 'povzbuzující', 'osvěžující', 'uvolňující',\n",
    "            'uklidňující', 'příznivý', 'konstruktivní', 'produktivní', 'perspektivní',\n",
    "            'slibný', 'nadějný', 'obohacující', 'vzrušující', 'úchvatný', 'impozantní', \n",
    "            'působivý', 'přesvědčivý', 'vítaný', 'populární', 'oblíbený', 'milovaný',\n",
    "            'oceňovaný', 'oslavovaný', 'vyzdvihovaný', 'vyžadovaný', 'potřebný', 'žádoucí',\n",
    "            'velmi', 'skvěle', 'nadšení', 'nadšený', 'radostný', 'vylepšený', 'přelomový',\n",
    "            'úžasně', 'nadmíru', 'mimořádně', 'výjimečně', 'srdečně', 'ideální', 'dobře'\n",
    "        ]\n",
    "        \n",
    "        self.negative_words = [\n",
    "            'špatný', 'negativní', 'problém', 'potíž', 'selhání', 'prohra', 'ztráta', 'pokles',\n",
    "            'krize', 'konflikt', 'smrt', 'válka', 'nehoda', 'tragédie', 'nebezpečí', 'zhoršení',\n",
    "            'škoda', 'nízký', 'horší', 'nejhorší', 'slabý', 'nepříznivý', 'riziko', 'hrozba',\n",
    "            'kritický', 'závažný', 'obtížný', 'těžký', 'násilí', 'strach', 'obavy', 'útok',\n",
    "            'katastrofa', 'pohroma', 'neštěstí', 'destrukce', 'zničení', 'zkáza', 'porážka',\n",
    "            'kolaps', 'pád', 'děsivý', 'hrozný', 'strašný', 'příšerný', 'otřesný', 'hrozivý',\n",
    "            'znepokojivý', 'alarmující', 'ohavný', 'odpudivý', 'nechutný', 'odporný', 'krutý',\n",
    "            'brutální', 'agresivní', 'surový', 'barbarský', 'divoký', 'vražedný', 'smrtící',\n",
    "            'jedovatý', 'toxický', 'škodlivý', 'ničivý', 'zničující', 'fatální', 'smrtelný',\n",
    "            'zoufalý', 'beznadějný', 'bezmocný', 'deprimující', 'skličující', 'depresivní',\n",
    "            'smutný', 'bolestný', 'trýznivý', 'traumatický', 'poškozený', 'rozbitý', 'zlomený',\n",
    "            'naštvaný', 'rozzlobený', 'rozzuřený', 'rozhořčený', 'nenávistný', 'nepřátelský',\n",
    "            'odmítavý', 'podvodný', 'klamavý', 'lživý', 'falešný', 'neetický', 'nemorální',\n",
    "            'zkorumpovaný', 'zkažený', 'prohnilý', 'bezcenný', 'zbytečný', 'marný', 'bídný',\n",
    "            'ubohý', 'žalostný', 'nedostatečný', 'průměrný', 'nudný', 'nezajímavý', 'nezáživný',\n",
    "            'bohužel', 'žel', 'naneštěstí', 'nešťastný', 'narušený', 'znechucený', 'zraněný',\n",
    "            'zraněno', 'utrpení', 'trápení', 'vážné', 'vážně', 'kriticky', 'drasticky', 'hrozně',\n",
    "            'selhal', 'selhala', 'nepovedlo', 'nefunguje', 'chyba', 'nefunkční', 'rozpadlý'\n",
    "        ]\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Vytvoření modelu neuronové sítě\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            # Embedding vrstva převádí tokeny na vektory\n",
    "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_length),\n",
    "            \n",
    "            # Bidirectional LSTM pro zachycení kontextu v obou směrech\n",
    "            Bidirectional(LSTM(128, return_sequences=True)),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Druhá LSTM vrstva\n",
    "            Bidirectional(LSTM(64)),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Plně propojené vrstvy\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Výstupní vrstva - 3 třídy: negativní, neutrální, pozitivní\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Kompilace modelu\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def fit(self, texts, labels, validation_split=0.2, epochs=15, batch_size=64):\n",
    "        \"\"\"\n",
    "        Trénování modelu neuronové sítě\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Seznam textů pro trénování\n",
    "            labels (list): Seznam labelů (0=negativní, 1=neutrální, 2=pozitivní)\n",
    "            validation_split (float): Část dat pro validaci\n",
    "            epochs (int): Počet epoch tréninku\n",
    "            batch_size (int): Velikost dávky\n",
    "            \n",
    "        Returns:\n",
    "            dict: Výsledky tréninku\n",
    "        \"\"\"\n",
    "        # Tokenizace textů\n",
    "        logger.info(\"Tokenizace textů...\")\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        \n",
    "        # Dělení dat na trénovací a validační\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            padded_sequences, np.array(labels),\n",
    "            test_size=validation_split,\n",
    "            random_state=42,\n",
    "            stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Vytvoření modelu, pokud ještě neexistuje\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "            logger.info(f\"Model vytvořen s architekturou:\")\n",
    "            self.model.summary(print_fn=lambda x: logger.info(x))\n",
    "        \n",
    "        # Callbacks pro zlepšení tréninku\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001),\n",
    "            ModelCheckpoint(\n",
    "                filepath='best_model.h5',\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                mode='max'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Trénink modelu\n",
    "        logger.info(f\"Začátek tréninku na {len(X_train)} vzorcích, validace na {len(X_val)} vzorcích...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Načtení nejlepšího modelu\n",
    "        if os.path.exists('best_model.h5'):\n",
    "            self.model = tf.keras.models.load_model('best_model.h5')\n",
    "            logger.info(\"Načten nejlepší model podle přesnosti validace\")\n",
    "        \n",
    "        # Vyhodnocení modelu\n",
    "        val_loss, val_acc = self.model.evaluate(X_val, y_val, verbose=0)\n",
    "        \n",
    "        # Predikce pro další metriky\n",
    "        y_pred = np.argmax(self.model.predict(X_val), axis=1)\n",
    "        confusion = confusion_matrix(y_val, y_pred)\n",
    "        report = classification_report(y_val, y_pred, target_names=self.labels, output_dict=True)\n",
    "        \n",
    "        if os.path.exists('best_model.h5'):\n",
    "            os.remove('best_model.h5')  # Odstraníme dočasný soubor\n",
    "        \n",
    "        # Detailní výsledky\n",
    "        results = {\n",
    "            'accuracy': val_acc,\n",
    "            'loss': val_loss,\n",
    "            'confusion_matrix': confusion.tolist(),\n",
    "            'classification_report': report,\n",
    "            'training_history': {\n",
    "                'accuracy': self.history.history['accuracy'],\n",
    "                'val_accuracy': self.history.history['val_accuracy'],\n",
    "                'loss': self.history.history['loss'],\n",
    "                'val_loss': self.history.history['val_loss']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"\n",
    "        Predikce sentimentu pro dané texty\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Seznam textů k analýze\n",
    "            \n",
    "        Returns:\n",
    "            list: Seznam predikovaných tříd (0=negativní, 1=neutrální, 2=pozitivní)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model není natrénován. Nejprve zavolejte metodu fit().\")\n",
    "        \n",
    "        # Tokenizace a padding textů\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        \n",
    "        # Predikce\n",
    "        predictions = self.model.predict(padded_sequences)\n",
    "        return np.argmax(predictions, axis=1).tolist()\n",
    "    \n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"\n",
    "        Predikce pravděpodobností sentimentu pro dané texty\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Seznam textů k analýze\n",
    "            \n",
    "        Returns:\n",
    "            list: Seznam vektorů pravděpodobností tříd\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model není natrénován. Nejprve zavolejte metodu fit().\")\n",
    "        \n",
    "        # Tokenizace a padding textů\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        \n",
    "        # Predikce pravděpodobností\n",
    "        return self.model.predict(padded_sequences).tolist()\n",
    "    \n",
    "    def extract_sentiment_features(self, texts):\n",
    "        \"\"\"\n",
    "        Extrakce příznaků souvisejících se sentimentem (pro kompatibilitu s předchozí verzí)\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Seznam textů k analýze\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame s extrahovanými příznaky\n",
    "        \"\"\"\n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # Počítání pozitivních a negativních slov\n",
    "        features['positive_word_count'] = [\n",
    "            sum(1 for word in text.lower().split() if word in self.positive_words) \n",
    "            for text in tqdm(texts, desc=\"Counting positive words\")\n",
    "        ]\n",
    "        \n",
    "        features['negative_word_count'] = [\n",
    "            sum(1 for word in text.lower().split() if word in self.negative_words)\n",
    "            for text in tqdm(texts, desc=\"Counting negative words\")\n",
    "        ]\n",
    "        \n",
    "        # Sentiment ratio (positive vs negative)\n",
    "        features['sentiment_ratio'] = (features['positive_word_count'] + 1) / (features['negative_word_count'] + 1)\n",
    "        \n",
    "        # Text length features\n",
    "        features['text_length'] = [len(text) for text in texts]\n",
    "        features['word_count'] = [len(text.split()) for text in texts]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def explain_prediction(self, text):\n",
    "        \"\"\"\n",
    "        Vysvětlení predikce sentimentu pro daný text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text k analýze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Vysvětlení predikce\n",
    "        \"\"\"\n",
    "        # Počítání pozitivních a negativních slov\n",
    "        positive_words_found = [word for word in text.lower().split() if word in self.positive_words]\n",
    "        negative_words_found = [word for word in text.lower().split() if word in self.negative_words]\n",
    "        \n",
    "        positive_word_count = len(positive_words_found)\n",
    "        negative_word_count = len(negative_words_found)\n",
    "        \n",
    "        # Výpočet poměru sentimentu\n",
    "        sentiment_ratio = (positive_word_count + 1) / (negative_word_count + 1)\n",
    "        \n",
    "        # Predikce\n",
    "        sentiment_id = self.predict([text])[0]\n",
    "        sentiment = self.labels[sentiment_id]\n",
    "        \n",
    "        # Confidence - pravděpodobnost predikce\n",
    "        proba = self.predict_proba([text])[0]\n",
    "        confidence = proba[sentiment_id]\n",
    "        \n",
    "        # Věty v textu\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        # Analyzovat sentiment jednotlivých vět\n",
    "        sentence_sentiments = []\n",
    "        if len(sentences) > 1:\n",
    "            for sentence in sentences:\n",
    "                if len(sentence.strip()) > 5:  # Pouze smysluplné věty\n",
    "                    sent_id = self.predict([sentence])[0]\n",
    "                    sentence_sentiments.append((sentence, self.labels[sent_id]))\n",
    "        \n",
    "        # Vytvoření vysvětlení\n",
    "        if sentiment == 'positive':\n",
    "            if positive_word_count > 0:\n",
    "                reason = f\"Text obsahuje pozitivní slova jako: {', '.join(positive_words_found[:5])}\"\n",
    "            else:\n",
    "                reason = \"Text má celkově pozitivní tón, i když neobsahuje konkrétní pozitivní slova z našeho slovníku.\"\n",
    "        elif sentiment == 'negative':\n",
    "            if negative_word_count > 0:\n",
    "                reason = f\"Text obsahuje negativní slova jako: {', '.join(negative_words_found[:5])}\"\n",
    "            else:\n",
    "                reason = \"Text má celkově negativní tón, i když neobsahuje konkrétní negativní slova z našeho slovníku.\"\n",
    "        else:\n",
    "            reason = \"Text obsahuje vyváženou směs pozitivních a negativních slov nebo neobsahuje dostatek slov s emočním nábojem.\"\n",
    "        \n",
    "        # Určení sentiment skóre\n",
    "        sentiment_score = (positive_word_count - negative_word_count) / max(len(text.split()), 1) * 10\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'predicted_sentiment': sentiment,\n",
    "            'confidence': confidence,\n",
    "            'sentiment_score': sentiment_score,\n",
    "            'positive_words': positive_words_found[:10],\n",
    "            'negative_words': negative_words_found[:10],\n",
    "            'positive_word_count': positive_word_count,\n",
    "            'negative_word_count': negative_word_count,\n",
    "            'word_count': len(text.split()),\n",
    "            'sentiment_ratio': sentiment_ratio,\n",
    "            'reason': reason,\n",
    "            'sentence_analysis': sentence_sentiments[:5]  # Omezení na prvních 5 vět\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self, figsize=(12, 5)):\n",
    "        \"\"\"\n",
    "        Vykreslení historie tréninku\n",
    "        \n",
    "        Args:\n",
    "            figsize (tuple): Velikost grafu\n",
    "        \"\"\"\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Historie tréninku není k dispozici. Nejprve zavolejte metodu fit().\")\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history.history['loss'], label='Train Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Ensure figures directory exists - FIXED VERSION\n",
    "        figures_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'reports', 'figures')\n",
    "        os.makedirs(figures_dir, exist_ok=True)\n",
    "        \n",
    "        plt.savefig(os.path.join(figures_dir, 'neural_sentiment_training_history.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    def save_model(self, model_dir):\n",
    "        \"\"\"\n",
    "        Uložení modelu na disk\n",
    "        \n",
    "        Args:\n",
    "            model_dir (str): Cesta k adresáři pro uložení modelu\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model není natrénován. Nejprve zavolejte metodu fit().\")\n",
    "        \n",
    "        # Vytvořit adresář, pokud neexistuje\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Uložit model neuronové sítě s .keras příponou (místo adresáře nn_model)\n",
    "        tf_model_path = os.path.join(model_dir, 'nn_model.keras')\n",
    "        if os.path.exists(tf_model_path):\n",
    "            os.remove(tf_model_path)  # Odstranit existující soubor modelu\n",
    "            \n",
    "        self.model.save(tf_model_path)\n",
    "        logger.info(f\"Model uložen do {tf_model_path}\")\n",
    "        \n",
    "        # Uložit tokenizer\n",
    "        with open(os.path.join(model_dir, 'tokenizer.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        \n",
    "        # Uložit lexikony\n",
    "        with open(os.path.join(model_dir, 'lexicons.pkl'), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'positive_words': self.positive_words,\n",
    "                'negative_words': self.negative_words\n",
    "            }, f)\n",
    "        \n",
    "        # Uložit model info\n",
    "        model_info = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'max_length': self.max_length,\n",
    "            'labels': self.labels\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(model_dir, 'model_info.pkl'), 'wb') as f:\n",
    "            pickle.dump(model_info, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, model_dir):\n",
    "        \"\"\"\n",
    "        Načtení modelu z disku\n",
    "        \n",
    "        Args:\n",
    "            model_dir (str): Cesta k adresáři s uloženým modelem\n",
    "            \n",
    "        Returns:\n",
    "            NeuralSentimentAnalyzer: Načtený model\n",
    "        \"\"\"\n",
    "        # Načíst model info\n",
    "        with open(os.path.join(model_dir, 'model_info.pkl'), 'rb') as f:\n",
    "            model_info = pickle.load(f)\n",
    "        \n",
    "        # Vytvořit instanci\n",
    "        instance = cls(\n",
    "            vocab_size=model_info['vocab_size'],\n",
    "            embedding_dim=model_info['embedding_dim'],\n",
    "            max_length=model_info['max_length']\n",
    "        )\n",
    "        \n",
    "        # Načíst toknenizer\n",
    "        with open(os.path.join(model_dir, 'tokenizer.pkl'), 'rb') as f:\n",
    "            instance.tokenizer = pickle.load(f)\n",
    "        \n",
    "        # Načíst model neuronové sítě\n",
    "        instance.model = tf.keras.models.load_model(os.path.join(model_dir, 'nn_model'))\n",
    "        \n",
    "        # Načíst lexikony\n",
    "        with open(os.path.join(model_dir, 'lexicons.pkl'), 'rb') as f:\n",
    "            lexicons = pickle.load(f)\n",
    "            instance.positive_words = lexicons['positive_words']\n",
    "            instance.negative_words = lexicons['negative_words']\n",
    "        \n",
    "        # Načíst labely\n",
    "        instance.labels = model_info['labels']\n",
    "        \n",
    "        logger.info(f\"Model načten z {model_dir}\")\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pomocné funkce a třídy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# custom JSON encoder to handle numpy types\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer, np.floating, np.bool_)):\n",
    "            return obj.item()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, figsize=(10, 8), cmap='Blues'):\n",
    "    \"\"\"\n",
    "    plot confusion matrix\n",
    "    \n",
    "    args:\n",
    "        cm (array): confusion matrix\n",
    "        class_names (list): list of class names\n",
    "        figsize (tuple): figure size\n",
    "        cmap (str): colormap\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Ensure figures directory exists\n",
    "    figures_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'reports', 'figures')\n",
    "    os.makedirs(figures_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(figures_dir, 'neural_sentiment_confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_class_distribution(class_dist, class_names, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    plot class distribution\n",
    "    \n",
    "    args:\n",
    "        class_dist (array): class distribution\n",
    "        class_names (list): list of class names\n",
    "        figsize (tuple): figure size\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.bar(class_names, class_dist)\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Ensure figures directory exists\n",
    "    figures_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'reports', 'figures')\n",
    "    os.makedirs(figures_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(figures_dir, 'neural_sentiment_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def create_balanced_dataset(df, column, min_samples=None, max_samples=None):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset with similar number of samples per class\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        column (str): Column name with class labels\n",
    "        min_samples (int): Minimum samples per class (if None, uses minimum class count)\n",
    "        max_samples (int): Maximum samples per class (if None, uses minimum class count)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Balanced dataframe\n",
    "    \"\"\"\n",
    "    class_counts = df[column].value_counts()\n",
    "    min_class_count = class_counts.min()\n",
    "    \n",
    "    if min_samples is None:\n",
    "        min_samples = min_class_count\n",
    "    \n",
    "    if max_samples is None:\n",
    "        max_samples = min_class_count\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    \n",
    "    for class_label, count in class_counts.items():\n",
    "        class_df = df[df[column] == class_label]\n",
    "        \n",
    "        # If class has fewer samples than min_samples, oversample\n",
    "        if count < min_samples:\n",
    "            class_df = class_df.sample(min_samples, replace=True, random_state=42)\n",
    "        # If class has more samples than max_samples, undersample\n",
    "        elif count > max_samples:\n",
    "            class_df = class_df.sample(max_samples, replace=False, random_state=42)\n",
    "        \n",
    "        balanced_dfs.append(class_df)\n",
    "    \n",
    "    return pd.concat(balanced_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hlavní funkce pro trénování modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    main function for training and evaluating the sentiment analyzer\n",
    "    \"\"\"\n",
    "    # create output directories if they don't exist\n",
    "    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    \n",
    "    model_dir = os.path.join(project_root, 'models', 'neural_sentiment_analyzer')\n",
    "    for directory in [model_dir, 'reports/models', 'reports/figures']:\n",
    "        dir_path = os.path.join(project_root, directory)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        logger.info(f\"ensuring directory exists: {dir_path}\")\n",
    "    \n",
    "    # load preprocessed data\n",
    "    processed_data_path = os.path.join(project_root, 'data', 'processed', 'articles_processed.csv')\n",
    "    \n",
    "    if not os.path.exists(processed_data_path):\n",
    "        logger.error(f\"Preprocessed data file not found: {processed_data_path}\")\n",
    "        logger.error(\"Run data_preparation.py first\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loading preprocessed data from {processed_data_path}\")\n",
    "    df = pd.read_csv(processed_data_path)\n",
    "    \n",
    "    # check if required columns exist\n",
    "    required_columns = ['Content', 'Title']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        logger.error(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "        return\n",
    "    \n",
    "    # basic info about data\n",
    "    logger.info(f\"Loaded {len(df)} articles\")\n",
    "    \n",
    "    # Sample data if it's too large to prevent memory issues\n",
    "    max_samples = 3000  # Maximum number of samples to use\n",
    "    if len(df) > max_samples:\n",
    "        logger.info(f\"Dataset is large ({len(df)} samples), using a random sample of {max_samples}\")\n",
    "        df = df.sample(max_samples, random_state=42)\n",
    "        logger.info(f\"Using {len(df)} samples for training\")\n",
    "    \n",
    "    # combine title and content for better analysis\n",
    "    df['Text'] = df['Title'] + ' ' + df['Content']\n",
    "    \n",
    "    # manually create sentiment features for seeding the sentiment labels\n",
    "    logger.info(\"Creating seed sentiment labels based on keyword analysis...\")\n",
    "    \n",
    "    # Keywords for sentiment analysis\n",
    "    positive_keywords = [\n",
    "        'dobrý', 'skvělý', 'výborný', 'pozitivní', 'úspěch', 'radost', 'krásný', 'příjemný',\n",
    "        'štěstí', 'spokojený', 'výhra', 'zisk', 'růst', 'lepší', 'nejlepší', 'zlepšení',\n",
    "        'vynikající', 'fantastický', 'báječný', 'prospěšný', 'podpora', 'nadějný'\n",
    "    ]\n",
    "    \n",
    "    negative_keywords = [\n",
    "        'špatný', 'negativní', 'problém', 'potíž', 'selhání', 'prohra', 'ztráta', 'pokles',\n",
    "        'krize', 'konflikt', 'smrt', 'válka', 'nehoda', 'tragédie', 'nebezpečí', 'zhoršení',\n",
    "        'škoda', 'horší', 'nejhorší', 'slabý', 'riziko', 'hrozba', 'kritický', 'strach'\n",
    "    ]\n",
    "    \n",
    "    # Create a function to assign sentiment based on keywords\n",
    "    def assign_seed_sentiment(text):\n",
    "        text = text.lower()\n",
    "        pos_count = sum(1 for word in positive_keywords if word in text)\n",
    "        neg_count = sum(1 for word in negative_keywords if word in text)\n",
    "        \n",
    "        # Return sentiment class (0: negative, 1: neutral, 2: positive)\n",
    "        if pos_count > neg_count + 1:\n",
    "            return 2  # positive\n",
    "        elif neg_count > pos_count + 1:\n",
    "            return 0  # negative\n",
    "        else:\n",
    "            return 1  # neutral\n",
    "    \n",
    "    # Initialize seed_sentiment column with automatic sentiment analysis\n",
    "    logger.info(\"Assigning initial sentiment labels...\")\n",
    "    df['seed_sentiment'] = df['Text'].apply(assign_seed_sentiment)\n",
    "    \n",
    "    # Check distribution of seed sentiments\n",
    "    seed_distribution = df['seed_sentiment'].value_counts()\n",
    "    logger.info(f\"Initial sentiment distribution: {seed_distribution.to_dict()}\")\n",
    "    \n",
    "    # Balance the dataset to improve training\n",
    "    logger.info(\"Balancing dataset...\")\n",
    "    balanced_df = create_balanced_dataset(df, 'seed_sentiment', \n",
    "                                         min_samples=min(500, seed_distribution.min()),\n",
    "                                         max_samples=1500)\n",
    "    \n",
    "    # Check balanced distribution\n",
    "    balanced_distribution = balanced_df['seed_sentiment'].value_counts()\n",
    "    logger.info(f\"Balanced sentiment distribution: {balanced_distribution.to_dict()}\")\n",
    "    \n",
    "    # train neural sentiment model\n",
    "    logger.info(\"Training neural sentiment model...\")\n",
    "    \n",
    "    # Initialize model\n",
    "    neural_analyzer = NeuralSentimentAnalyzer(\n",
    "        vocab_size=20000,      # Slovník 20K nejčastějších slov\n",
    "        embedding_dim=200,     # Dimenze embeddings\n",
    "        max_length=500         # Maximální délka článku v tokenech\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_data = balanced_df['Text'].values\n",
    "    train_labels = balanced_df['seed_sentiment'].values\n",
    "    \n",
    "    logger.info(f\"Training neural sentiment model on {len(train_data)} samples...\")\n",
    "    \n",
    "    # Train with progress bar\n",
    "    results = neural_analyzer.fit(\n",
    "        train_data, \n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        epochs=20,          # Maximum epochs\n",
    "        batch_size=32      # Batch size\n",
    "    )\n",
    "    \n",
    "    # Plot training history - FIXED FUNCTION\n",
    "    neural_analyzer.plot_training_history()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(\n",
    "        np.array(results['confusion_matrix']),\n",
    "        neural_analyzer.labels,\n",
    "        figsize=(10, 8)\n",
    "    )\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plot_class_distribution(\n",
    "        balanced_distribution.values,\n",
    "        neural_analyzer.labels,\n",
    "        figsize=(8, 6)\n",
    "    )\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results_path = os.path.join(\n",
    "        project_root, 'reports', 'models', f'neural_sentiment_analyzer_results.json'\n",
    "    )\n",
    "    \n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    # Save model\n",
    "    neural_analyzer.save_model(model_dir)\n",
    "    logger.info(f\"Neural sentiment model saved to {model_dir}\")\n",
    "    \n",
    "    # Create main model directory\n",
    "    main_model_dir = os.path.join(project_root, 'models', 'enhanced_sentiment_analyzer')\n",
    "    os.makedirs(main_model_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy files from neural model to main directory for compatibility\n",
    "    try:\n",
    "        # Create symlink or copy files\n",
    "        for subdir, dirs, files in os.walk(model_dir):\n",
    "            for file in files:\n",
    "                src_path = os.path.join(subdir, file)\n",
    "                rel_path = os.path.relpath(src_path, model_dir)\n",
    "                dst_path = os.path.join(main_model_dir, rel_path)\n",
    "                \n",
    "                # Create subdirectories if needed\n",
    "                os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "                \n",
    "                # Copy file\n",
    "                import shutil\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "        \n",
    "        # Copy model_info.pkl with adjusted settings\n",
    "        with open(os.path.join(model_dir, 'model_info.pkl'), 'rb') as f:\n",
    "            model_info = pickle.load(f)\n",
    "        \n",
    "        model_info['model_type'] = 'neural'  # Add model type\n",
    "        \n",
    "        with open(os.path.join(main_model_dir, 'model_info.pkl'), 'wb') as f:\n",
    "            pickle.dump(model_info, f)\n",
    "        \n",
    "        logger.info(f\"Model files copied to main directory {main_model_dir}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error copying model files: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Training completed. Results saved to reports/models/\")\n",
    "    \n",
    "    # test sentiment analyzer on a few sample articles\n",
    "    logger.info(\"Testing neural sentiment analyzer on sample articles...\")\n",
    "    \n",
    "    # load trained model\n",
    "    analyzer = NeuralSentimentAnalyzer.load_model(model_dir)\n",
    "    \n",
    "    # sample articles\n",
    "    sample_idx = np.random.randint(0, len(df), size=5)\n",
    "    samples = df.iloc[sample_idx]\n",
    "    \n",
    "    for _, article in samples.iterrows():\n",
    "        text = article['Text']\n",
    "        sentiment_id = analyzer.predict([text])[0]\n",
    "        \n",
    "        logger.info(f\"Title: {article['Title'][:50]}...\")\n",
    "        logger.info(f\"Predicted sentiment: {analyzer.labels[sentiment_id]}\")\n",
    "        \n",
    "        # Get detailed explanation\n",
    "        explanation = analyzer.explain_prediction(text)\n",
    "        positive_words = explanation['positive_words']\n",
    "        negative_words = explanation['negative_words']\n",
    "        ratio = explanation['sentiment_ratio']\n",
    "        \n",
    "        logger.info(f\"Positive words: {positive_words}\")\n",
    "        logger.info(f\"Negative words: {negative_words}\")\n",
    "        logger.info(f\"Confidence: {explanation['confidence']:.2f}\")\n",
    "        logger.info(f\"Ratio: {ratio:.2f}\")\n",
    "        logger.info(f\"Reason: {explanation['reason']}\")\n",
    "        logger.info(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spuštění trénování modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check for GPU availability\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            logger.info(f\"GPU is available: {gpus}\")\n",
    "            # Set memory growth to avoid memory allocation errors\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        else:\n",
    "            logger.info(\"No GPU found, using CPU for training\")\n",
    "        \n",
    "        # Run main function\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Unexpected error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
